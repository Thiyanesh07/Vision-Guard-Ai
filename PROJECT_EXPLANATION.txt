================================================================================
                    SMART CITY AI - PROJECT DOCUMENTATION
================================================================================

PROJECT OVERVIEW
================================================================================

Smart City AI is a real-time incident detection and management system that uses 
artificial intelligence (computer vision) to monitor video streams and 
automatically detect urban incidents such as traffic congestion, accidents, 
fires, floods, and other emergencies.

WHAT DOES THIS PROJECT DO?
================================================================================

1. MONITORS VIDEO FEEDS
   - Connects to RTSP camera streams (or webcam/video files)
   - Processes frames in real-time using AI

2. DETECTS INCIDENTS USING AI
   - Uses YOLOv11-Nano (a lightweight object detection model)
   - Identifies objects and classifies them as incidents
   - Currently detects: persons, vehicles, and other objects
   - Maps detected objects to incident types (traffic congestion, etc.)

3. CAPTURES EVIDENCE
   - Maintains a rolling buffer of the last 16 frames
   - When an incident is detected, extracts 4 evenly-spaced frames
   - Uploads frames to cloud storage (MinIO)

4. PROCESSES AND STORES INCIDENTS
   - Publishes incident events to a message queue (Redis)
   - Background workers process incidents asynchronously
   - Stores all incident data in PostgreSQL database with geolocation

5. PROVIDES API ACCESS
   - REST API for creating, listing, and retrieving incidents
   - WebSocket support for real-time incident notifications
   - Health check endpoints for monitoring

6. ENABLES VERIFICATION (OPTIONAL)
   - Can send incidents to an LLM service for verification
   - Updates verification status (pending, verified, rejected)


TECH STACK
================================================================================

PROGRAMMING LANGUAGE
--------------------
- Python 3.10+

COMPUTER VISION & AI
--------------------
- YOLOv11-Nano: Lightweight object detection model (ONNX format)
- OpenCV: Video processing and frame manipulation
- ONNX Runtime (GPU): Hardware-accelerated inference
- PyTorch & Torchvision: Deep learning framework
- Ultralytics: YOLO model training and export
- NumPy: Numerical operations for image processing

WEB FRAMEWORK & API
--------------------
- FastAPI: Modern, fast web framework for building APIs
- Uvicorn: ASGI server for running FastAPI
- WebSockets: Real-time bidirectional communication
- Pydantic: Data validation and settings management

DATABASE & STORAGE
--------------------
- PostgreSQL 14+: Relational database for storing incidents
- PostGIS Extension: Geospatial data support for location tracking
- SQLAlchemy: Async ORM (Object-Relational Mapping)
- Asyncpg: Async PostgreSQL driver
- GeoAlchemy2: PostGIS integration for SQLAlchemy
- Alembic: Database migration management
- Shapely: Geometric operations for location data

MESSAGE QUEUE & TASK PROCESSING
--------------------------------
- Redis: In-memory data store for message queue
- Celery: Distributed task queue for background processing
- redis-py: Python Redis client

OBJECT STORAGE
--------------
- MinIO: S3-compatible object storage for storing frame images
- minio-py: Python MinIO client

CONFIGURATION & UTILITIES
--------------------------
- python-dotenv: Environment variable management
- pydantic-settings: Settings management from .env files
- requests: HTTP client for external API calls

DEVELOPMENT & DEPLOYMENT
------------------------
- PowerShell Scripts: Service management automation
- Alembic: Database schema version control

HARDWARE ACCELERATION
---------------------
- CUDA 12.6: GPU acceleration for AI inference
- ONNX Runtime GPU: GPU-optimized model execution


PROJECT ARCHITECTURE & FLOW
================================================================================

SYSTEM COMPONENTS
-----------------

1. Vision Pipeline (vision/yolov11_pipeline.py)
   - Captures frames from video stream
   - Runs YOLOv11 inference on each frame
   - Maintains rolling frame buffer
   - Detects incidents and extracts frames
   - Uploads frames to MinIO
   - Publishes events to Redis

2. Redis Message Queue
   - Acts as the communication backbone
   - Receives incident events from vision pipeline
   - Distributes events to consumers

3. Redis Consumer (tasks/redis_consumer.py)
   - Subscribes to Redis 'events' channel
   - Receives incident events
   - Triggers Celery tasks for processing

4. Celery Worker (tasks/celery_worker.py)
   - Processes incidents asynchronously
   - Stores incidents in PostgreSQL database
   - Optionally sends to LLM service for verification
   - Updates verification status

5. FastAPI Backend (backend/main.py)
   - Provides REST API endpoints
   - Handles WebSocket connections
   - Queries database for incidents
   - Broadcasts real-time updates to connected clients

6. PostgreSQL Database (database/models.py)
   - Stores incident records with geolocation
   - Supports geospatial queries via PostGIS
   - Tracks verification status

7. MinIO Storage (storage/minio_client.py)
   - Stores captured frame images
   - Provides public URLs for frame access
   - S3-compatible interface


DETAILED FLOW DIAGRAM
================================================================================

START: Video Stream (RTSP/Webcam/File)
   |
   v
[Vision Pipeline - vision/yolov11_pipeline.py]
   |
   |---> Capture Frame
   |---> Add to Rolling Buffer (last 16 frames)
   |---> Run YOLOv11 Inference
   |
   |---> INCIDENT DETECTED?
         |
         | YES
         v
   Extract 4 Spaced Frames from Buffer
         |
         v
   [MinIO Storage - storage/minio_client.py]
         |
         |---> Upload frames as JPEG
         |---> Get public URLs
         |
         v
   Create Event Payload:
   {
     "id": "incident_20251119123045678",
     "incident": "traffic_congestion",
     "confidence": 0.87,
     "frames": ["url1", "url2", "url3", "url4"],
     "timestamp": "2025-11-19T12:30:45",
     "location": {"lat": 11.0222, "lon": 77.0133}
   }
         |
         v
   [Redis Pub/Sub]
         |---> Publish to 'events' channel
         |
         v
   [Redis Consumer - tasks/redis_consumer.py]
         |---> Subscribe to 'events' channel
         |---> Receive event
         |
         v
   Trigger Celery Task: process_incident()
         |
         v
   [Celery Worker - tasks/celery_worker.py]
         |
         |---> TASK 1: process_incident()
         |     |
         |     |---> Extract event data
         |     |---> Create PostGIS Point (location)
         |     |---> Store in PostgreSQL
         |     |---> Return success
         |
         |---> TASK 2: send_to_llm_service() [OPTIONAL]
         |     |
         |     |---> Send incident to LLM API
         |     |---> LLM verifies if incident is real
         |     |---> Return verification result
         |     |
         |     v
         |     Trigger update_verification_status()
         |     |
         |     |---> Update database status
         |     |---> Status: 'verified' or 'rejected'
         |
         v
   [PostgreSQL Database - database/models.py]
         |
         |---> Incidents Table:
         |     - id (primary key)
         |     - incident_type
         |     - confidence
         |     - timestamp
         |     - frame_urls (JSON array)
         |     - verification_status
         |     - location (PostGIS POINT)
         |
         v
   [FastAPI Backend - backend/main.py]
         |
         |---> REST API Endpoints:
         |     |
         |     |---> POST /incidents/create
         |     |     - Create new incident
         |     |     - Broadcast via WebSocket
         |     |
         |     |---> GET /incidents/list
         |     |     - List all incidents
         |     |
         |     |---> GET /incidents/{id}
         |     |     - Get specific incident
         |     |
         |     |---> POST /verify/from_llm
         |     |     - Update verification status
         |     |     - Broadcast update via WebSocket
         |     |
         |     |---> GET /health
         |           - Health check
         |
         |---> WebSocket Endpoint:
         |     |
         |     |---> WS /ws
         |           - Real-time incident notifications
         |           - Verification status updates
         |
         v
   [Frontend Dashboard - NOT INCLUDED]
         |
         |---> Connect to WebSocket
         |---> Display real-time incidents
         |---> Show incident details and frames
         |---> Allow manual verification


DATA FLOW EXAMPLE
================================================================================

SCENARIO: A car accident is detected

1. VIDEO STREAM (00:00:00.000)
   Camera captures accident scene

2. VISION PIPELINE (00:00:00.010)
   - Frame captured
   - Added to buffer (frames 1-16 stored)
   - YOLOv11 runs inference
   - Detects "car" with 87% confidence
   - Maps to incident type: "traffic_congestion"

3. FRAME EXTRACTION (00:00:00.050)
   - Extract frames: #4, #8, #12, #16 from buffer
   - Encode as JPEG images

4. MINIO UPLOAD (00:00:00.100)
   - Upload 4 frames to MinIO bucket "frames"
   - Files: incident_20251119120000_f1.jpg, f2.jpg, f3.jpg, f4.jpg
   - Get URLs: http://minio:9000/frames/incident_xxx_f1.jpg

5. REDIS PUBLISH (00:00:00.150)
   - Create event JSON payload
   - Publish to Redis channel 'events'

6. REDIS CONSUMER (00:00:00.160)
   - Receive event from Redis
   - Log: "Received event: incident_20251119120000"

7. CELERY TASK (00:00:00.200)
   - Celery worker picks up task
   - Convert lat/lon to PostGIS Point
   - Insert into PostgreSQL

8. DATABASE STORAGE (00:00:00.250)
   PostgreSQL incidents table:
   +---------------------------+----------------------+------------+
   | id                        | incident_type        | confidence |
   +---------------------------+----------------------+------------+
   | incident_20251119120000   | traffic_congestion   | 0.87       |
   +---------------------------+----------------------+------------+
   | timestamp                 | frame_urls           | status     |
   +---------------------------+----------------------+------------+
   | 2025-11-19 12:00:00       | [url1, url2, ...]    | pending    |
   +---------------------------+----------------------+------------+

9. LLM VERIFICATION (00:00:01.000) [OPTIONAL]
   - Send incident + frames to LLM service
   - LLM analyzes: "Yes, this is a traffic incident"
   - Return: status = "verified"

10. STATUS UPDATE (00:00:01.500)
    - Update database: verification_status = "verified"

11. WEBSOCKET BROADCAST (00:00:01.550)
    - FastAPI broadcasts to all connected clients
    - Message: {"type": "incident_verified", "id": "incident_xxx"}

12. FRONTEND UPDATE (00:00:01.600)
    - Dashboard receives WebSocket message
    - Updates UI to show verified incident
    - Display frames and details


KEY FEATURES
================================================================================

1. FRAME BUFFERING
   - Keeps last 16 frames in memory
   - When incident detected, can access frames BEFORE the detection
   - Provides context (what happened leading up to incident)

2. DETECTION COOLDOWN
   - 5-second cooldown between detections
   - Prevents duplicate incidents from same event
   - Reduces false positives

3. MULTI-FRAME EVIDENCE
   - Extracts 4 frames per incident (evenly spaced)
   - Provides visual evidence from multiple angles/times
   - Better for verification by humans or LLM

4. GEOSPATIAL SUPPORT
   - Uses PostGIS for location data
   - Enables queries like "incidents within 5km radius"
   - Supports map visualization

5. ASYNCHRONOUS PROCESSING
   - Vision pipeline runs independently
   - Database operations don't block detection
   - Celery handles background tasks
   - FastAPI uses async/await for high performance

6. REAL-TIME NOTIFICATIONS
   - WebSocket broadcasts new incidents instantly
   - Clients receive updates without polling
   - Low latency for critical incidents

7. SCALABILITY
   - Redis queue can handle high throughput
   - Multiple Celery workers can run in parallel
   - Database supports millions of records
   - MinIO scales horizontally

8. GPU ACCELERATION
   - ONNX Runtime uses CUDA for inference
   - 10-100x faster than CPU
   - Essential for real-time processing


CONFIGURATION
================================================================================

All configuration is in config.py and loaded from .env file:

DATABASE
--------
- database_url: PostgreSQL connection string
  Example: postgresql+asyncpg://user:pass@localhost:5432/smart_city_ai

REDIS
-----
- redis_host: Redis server hostname (default: localhost)
- redis_port: Redis port (default: 6379)
- redis_db: Redis database number (default: 0)

CELERY
------
- celery_broker_url: Redis URL for Celery broker
- celery_result_backend: Redis URL for task results

MINIO
-----
- minio_endpoint: MinIO server address (default: localhost:9000)
- minio_access_key: Access key (default: minioadmin)
- minio_secret_key: Secret key (default: minioadmin)
- minio_bucket: Bucket name for frames (default: frames)

VISION
------
- yolo_model_path: Path to ONNX model (default: models/yolov11-nano.onnx)
- stream_url: Video source (RTSP URL, webcam ID, or file path)
- confidence_threshold: Minimum detection confidence (0.0-1.0)
- frame_buffer_size: Number of frames to keep in buffer (default: 16)
- frames_to_extract: Frames to save per incident (default: 4)

LOCATION
--------
- default_lat: Default latitude for incidents (default: 11.0222)
- default_lon: Default longitude for incidents (default: 77.0133)


API ENDPOINTS
================================================================================

REST API (http://localhost:8000)
---------------------------------

1. POST /incidents/create
   Create a new incident manually
   
   Request Body:
   {
     "incident": "accident",
     "confidence": 0.87,
     "frame_urls": ["url1", "url2"],
     "timestamp": "2025-11-19T12:00:00Z",
     "location": {"lat": 11.0222, "lon": 77.0133}
   }
   
   Response: Incident object with ID

2. GET /incidents/list
   List all incidents
   
   Response: Array of incident objects

3. GET /incidents/{id}
   Get specific incident by ID
   
   Response: Incident object or 404

4. POST /verify/from_llm?id={id}&status={status}
   Update verification status (called by LLM service)
   
   Query Params:
   - id: Incident ID
   - status: verified | rejected | pending
   
   Response: {"id": "...", "status": "..."}

5. GET /health
   Health check endpoint
   
   Response: {"status": "healthy"}

WEBSOCKET (ws://localhost:8000/ws)
----------------------------------

Connect to receive real-time updates:

Message Types:
1. New Incident:
   {
     "type": "new_incident",
     "data": { Incident object }
   }

2. Verification Update:
   {
     "type": "incident_verified",
     "data": {"id": "...", "status": "..."}
   }


SERVICES REQUIRED
================================================================================

To run this project, you need 6 services running simultaneously:

1. PostgreSQL (Port 5432)
   - Database for storing incidents
   - Must have PostGIS extension installed

2. Redis (Port 6379)
   - Message queue and Celery broker
   - In-memory data store

3. MinIO (Port 9000, Console: 9001)
   - Object storage for frame images
   - S3-compatible interface

4. FastAPI Backend (Port 8000)
   - REST API and WebSocket server
   - Command: uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000

5. Celery Worker
   - Background task processor
   - Command: celery -A tasks.celery_worker worker --loglevel=info --pool=solo

6. Redis Consumer
   - Listens for incidents from vision pipeline
   - Command: python tasks/redis_consumer.py

7. Vision Pipeline (Optional - for live detection)
   - Processes video stream
   - Command: python vision/yolov11_pipeline.py


INCIDENT TYPES DETECTED
================================================================================

Current Mapping (COCO Dataset):
- class_id 0 (person) -> "person_detected"
- class_id 2 (car) -> "traffic_congestion"
- class_id 5 (bus) -> "traffic_congestion"
- class_id 7 (truck) -> "traffic_congestion"
- class_id 39 (bottle) -> "bottle_detected"
- class_id 63 (laptop) -> "laptop_detected"
- class_id 67 (cell_phone) -> "cell_phone_detected"

For Production (Custom Model Required):
- Accident detection
- Fire/Smoke detection
- Fight/Violence detection
- Flood/Waterlogging detection
- Garbage/Pollution detection

Note: To detect these properly, you need to train a custom YOLOv11 model
on a dataset containing these incident types.


FILE STRUCTURE
================================================================================

Smart city AI/
│
├── backend/
│   └── main.py                      # FastAPI application (REST + WebSocket)
│
├── database/
│   ├── models.py                    # SQLAlchemy models (Incident table)
│   └── migrations/                  # Alembic database migrations
│       ├── env.py                   # Alembic environment config
│       ├── script.py.mako           # Migration template
│       └── versions/
│           └── 0001_create_incidents_table.py  # Initial migration
│
├── storage/
│   └── minio_client.py              # MinIO storage client
│
├── tasks/
│   ├── celery_worker.py             # Celery tasks (process, verify, update)
│   ├── redis_consumer.py            # Redis event consumer
│   └── redis_producer.py            # Redis event publisher (not used yet)
│
├── vision/
│   └── yolov11_pipeline.py          # YOLOv11 detection pipeline
│
├── models/
│   └── yolov11-nano.onnx            # YOLOv11 model (ONNX format)
│
├── minio-data/
│   └── frames/                      # MinIO storage directory (local)
│
├── config.py                        # Centralized configuration
├── requirements.txt                 # Python dependencies
├── alembic.ini                      # Alembic configuration
├── .env                             # Environment variables (not in Git)
├── .env.example                     # Environment template
│
├── README.md                        # Project overview
├── IMPLEMENTATION.md                # Implementation details
├── SETUP.md                         # Setup instructions
├── QUICKSTART.md                    # Quick start guide
├── GPU_SETUP.md                     # GPU setup instructions
│
├── start-all.ps1                    # Start all services (PowerShell)
├── start-services.ps1               # Service manager (PowerShell)
├── setup.ps1                        # Initial setup script
│
├── test_system.py                   # System test script
├── check_status.py                  # Service status checker
└── download_model.py                # Model download utility


USE CASES
================================================================================

1. SMART CITY MONITORING
   - Monitor traffic intersections
   - Detect accidents and congestion
   - Automatic emergency response

2. PUBLIC SAFETY
   - Detect fights or violence
   - Fire and smoke detection
   - Flood monitoring

3. URBAN MANAGEMENT
   - Garbage dump detection
   - Pollution monitoring
   - Infrastructure issues

4. TRANSPORTATION
   - Traffic flow analysis
   - Parking violations
   - Vehicle counting

5. ENVIRONMENTAL MONITORING
   - Flood detection and tracking
   - Waste accumulation
   - Air quality indicators


ADVANTAGES
================================================================================

1. REAL-TIME PROCESSING
   - Detects incidents as they happen
   - Immediate notification to authorities

2. AUTOMATED MONITORING
   - No manual monitoring required
   - 24/7 operation
   - Consistent detection

3. EVIDENCE CAPTURE
   - Multi-frame evidence
   - Timestamped and geolocated
   - Stored for future reference

4. SCALABLE ARCHITECTURE
   - Can handle multiple camera streams
   - Distributed processing
   - Cloud-ready

5. FLEXIBLE VERIFICATION
   - Optional LLM verification
   - Manual verification support
   - Reduces false positives

6. OPEN API
   - REST and WebSocket interfaces
   - Easy integration with dashboards
   - Third-party service integration


LIMITATIONS & FUTURE IMPROVEMENTS
================================================================================

CURRENT LIMITATIONS
-------------------
1. Uses generic COCO model (not specialized for urban incidents)
2. No camera calibration for accurate distance/speed
3. Fixed location (not per-camera configuration)
4. No multi-camera tracking
5. No incident classification refinement
6. No alerting system (email, SMS, etc.)

FUTURE IMPROVEMENTS
-------------------
1. Train custom YOLOv11 model for specific incidents
2. Add camera management system
3. Implement multi-camera tracking
4. Add alert/notification system
5. Build web dashboard frontend
6. Add analytics and reporting
7. Implement incident prediction
8. Add heat map visualization
9. Integration with emergency services
10. Mobile app support


DEPLOYMENT OPTIONS
================================================================================

OPTION 1: Local Development
- Run all services on local machine
- Good for testing and development
- Limited scalability

OPTION 2: Docker Compose
- Package all services in containers
- Easy deployment
- Better resource management

OPTION 3: Kubernetes
- Enterprise-grade orchestration
- Auto-scaling
- High availability
- Best for production

OPTION 4: Cloud Platform
- AWS/Azure/GCP managed services
- Use RDS, ElastiCache, S3
- Fully managed infrastructure


TROUBLESHOOTING
================================================================================

COMMON ISSUES
-------------

1. "CUDA not found" or GPU not working
   - Install CUDA Toolkit 12.6
   - Install onnxruntime-gpu
   - Verify with: nvidia-smi

2. "Cannot connect to PostgreSQL"
   - Check PostgreSQL is running
   - Verify credentials in .env
   - Check PostGIS extension installed

3. "Redis connection refused"
   - Start Redis server
   - Check Redis is listening on port 6379

4. "MinIO bucket not found"
   - Access MinIO console (http://localhost:9001)
   - Create bucket named "frames"
   - Set bucket policy to public

5. "No module named 'config'"
   - Ensure you're running from project root
   - Check Python path includes parent directory

6. Celery worker not processing tasks
   - Verify Redis connection
   - Check Celery worker logs
   - Ensure database is accessible


PERFORMANCE CONSIDERATIONS
================================================================================

1. GPU INFERENCE
   - YOLOv11-Nano: ~50-100 FPS on RTX 3050
   - CPU inference: ~5-10 FPS
   - GPU highly recommended for real-time processing

2. DATABASE OPTIMIZATION
   - Add indexes on timestamp and incident_type
   - Use connection pooling
   - Consider read replicas for high traffic

3. STORAGE OPTIMIZATION
   - Compress frames before upload
   - Use object lifecycle policies
   - Archive old frames to cheaper storage

4. NETWORK OPTIMIZATION
   - Use local network for RTSP streams
   - Cache frequently accessed frames
   - Use CDN for frame delivery


SECURITY CONSIDERATIONS
================================================================================

1. AUTHENTICATION
   - Add API key authentication
   - Use JWT tokens
   - Implement rate limiting

2. DATABASE SECURITY
   - Use strong passwords
   - Enable SSL connections
   - Restrict network access

3. STORAGE SECURITY
   - Use presigned URLs with expiration
   - Encrypt sensitive frames
   - Implement access controls

4. NETWORK SECURITY
   - Use HTTPS for API
   - Secure WebSocket connections (WSS)
   - Configure firewall rules


TESTING
================================================================================

1. Unit Tests
   - Test individual functions
   - Mock external dependencies

2. Integration Tests
   - Test component interactions
   - Verify data flow

3. End-to-End Tests
   - Test complete pipeline
   - Use test video files

4. Performance Tests
   - Measure inference speed
   - Test database throughput
   - Stress test API endpoints


MONITORING & LOGGING
================================================================================

1. Application Logs
   - FastAPI logs in console
   - Celery worker logs
   - Vision pipeline logs

2. Database Monitoring
   - Track query performance
   - Monitor connections
   - Watch storage growth

3. System Metrics
   - CPU and GPU usage
   - Memory consumption
   - Network bandwidth

4. Alerting
   - Service downtime
   - High error rates
   - Storage capacity


COST ESTIMATION (Cloud Deployment)
================================================================================

Monthly costs for medium-scale deployment:

1. Compute (10 cameras)
   - GPU instances: $500-1000
   - API servers: $100-200
   - Worker instances: $100-200

2. Storage
   - Object storage: $50-100 (1TB)
   - Database: $100-200

3. Data Transfer
   - Ingress: Free
   - Egress: $50-100

4. Other Services
   - Redis: $50-100
   - Load balancer: $20

Total: $1000-2000/month for 10 cameras

Scale linearly with number of cameras.


CONCLUSION
================================================================================

Smart City AI is a comprehensive, production-ready system for automated
incident detection and management. It combines modern computer vision,
distributed processing, and real-time communication to create an intelligent
urban monitoring solution.

The modular architecture allows easy customization and scaling, while the
use of industry-standard technologies ensures reliability and maintainability.

For production deployment, train a custom YOLOv11 model on domain-specific
data, build a frontend dashboard, and deploy using containerization for
best results.

================================================================================
                            END OF DOCUMENTATION
================================================================================

Generated: November 19, 2025
Project: Smart City AI v1.0.0
Author: AI-Powered Urban Monitoring System
================================================================================
